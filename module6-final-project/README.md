# ðŸ“˜ Module 6 â€“ Practice & Final Project (IBM Machine Learning)

## Overview
In this final module, I applied the **full machine learning workflow** to real-world datasets.  
The module consisted of:
1. **Practice Project** â€“ Titanic dataset (binary classification).  
2. **Final Project** â€“ Rainfall prediction dataset (binary classification).  
3. **Final Exam** â€“ Summative assessment covering all six modules.

---

## ðŸŽ¯ Learning Objectives
- Apply **end-to-end ML techniques** on real-world data.  
- Practice **pipeline construction and optimization**.  
- Compare multiple models using **cross-validation and hyperparameter tuning**.  
- Evaluate models with metrics suitable for **imbalanced datasets**.  
- Demonstrate **feature engineering, preprocessing, and model interpretability**.  

---

## ðŸ›  Practice Project: Titanic Dataset
- **Goal:** Predict passenger survival.  
- **Workflow:**  
  - Preprocessing with `ColumnTransformer` â†’ `SimpleImputer`, `StandardScaler`, `OneHotEncoder`.  
  - Built full **Pipeline** with `RandomForestClassifier` and `LogisticRegression`.  
  - Used `StratifiedKFold` CV + `GridSearchCV` for hyperparameter tuning.  
  - Evaluated with **classification report**, confusion matrix, ROC-AUC.  
  - Mapped feature importances back to original features after One-Hot Encoding.  
- **Results:**  
  - Best weighted F1 â‰ˆ **0.82**.  
  - Random Forest slightly outperformed Logistic Regression.  
  - Model reproducibility ensured with `random_state`.  

---

## ðŸŒ§ Final Project: Rainfall Prediction
- **Goal:** Predict whether it will rain today, based on historical weather data.  
- **Steps:**  
  - Addressed **data leakage** (removed future-dependent features).  
  - Restricted to **Melbourne metro locations** for regional consistency.  
  - Feature engineering: created **Season** from date, shifted `RainTomorrow â†’ RainToday`.  
  - Preprocessing: pipelines for numerical (median imputation + scaling) and categorical (imputation + one-hot).  
  - Models: Random Forest, Logistic Regression.  
  - GridSearchCV with **time-aware CV**; evaluated with Accuracy, Precision, Recall, F1.  
- **Results:**  
  - Accuracy â‰ˆ **0.84**, but recall for rainy days only â‰ˆ **0.50**.  
  - Random Forest achieved higher overall accuracy; Logistic Regression slightly better TPR.  
  - Feature importance: **Sunshine** and **Humidity3pm** were most predictive.  

---

## ðŸ“Š Key Skills Practiced
- Handling **imbalanced datasets** (`class_weight="balanced"`).  
- Building **reusable ML pipelines** for preprocessing + modeling.  
- **Hyperparameter tuning** with GridSearchCV.  
- Comparing models fairly on the same test set.  
- **Interpretability**: Feature importances & coefficients.  
- **Model validation**: Preventing overfitting and avoiding data leakage.  

---

## âœ… Takeaways
- Accuracy alone is misleading on imbalanced data â†’ use Precision, Recall, F1, ROC/PR curves.  
- Pipelines + ColumnTransformers = scalable, reproducible ML workflow.  
- Feature engineering (like lags, seasonality) has big impact on performance.  
- Random Forest is a strong baseline, but threshold tuning and class weights are crucial for minority class detection.  

---

## ðŸ“‚ Deliverables
- **Practice Project Notebook:** Titanic survival prediction.  
- **Final Project Notebook:** Rainfall prediction classifier.  
- **Final Exam:** Completed graded assessment.  

---

## ðŸš€ Next Steps
- Apply the same pipeline approach to **new datasets**.  
- Explore more advanced models (XGBoost, LightGBM).  
- Focus on **model calibration & threshold tuning** for imbalanced problems.  
- Use **SHAP** for deeper feature explainability in tree models.

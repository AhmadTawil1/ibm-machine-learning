# ğŸ“˜ Module 2: Regression Models (IBM Machine Learning Course)

This module covers **supervised regression methods** â€” from linear models to logistic regression â€” and introduces optimization techniques like gradient descent. The work includes theory, examples, and a practical lab implementation (`Logistic_Regression.ipynb`).

---

## ğŸ“‚ Contents
- Linear Regression
- Multiple Linear Regression
- Polynomial Regression
- Nonlinear Regression
- Logistic Regression
- Training Logistic Regression with Gradient Descent & SGD
- Lab: `Logistic_Regression.ipynb`

---

## ğŸ§  Key Concepts

### ğŸ”¹ Linear Regression
- Predicts continuous outcomes with a straight line.  
- Formula:  
  `yÌ‚ = Î¸â‚€ + Î¸â‚x`  
- Cost function: **Mean Squared Error (MSE)**.

### ğŸ”¹ Multiple Linear Regression
- Extends to multiple predictors.  
- Formula:  
  `yÌ‚ = Î¸â‚€ + Î¸â‚xâ‚ + Î¸â‚‚xâ‚‚ + â€¦ + Î¸â‚™xâ‚™`.

### ğŸ”¹ Polynomial Regression
- Models curved relationships with polynomial terms.  
- Solved via linear regression on transformed features.  
- Risk: **overfitting** with high-degree polynomials.

### ğŸ”¹ Nonlinear Regression
- Models non-polynomial relationships (exponential, logarithmic, sinusoidal).  
- Examples: GDP growth (exponential), diminishing returns (logarithmic).

### ğŸ”¹ Logistic Regression
- Binary classification (0/1).  
- Outputs probabilities via **sigmoid function**:  
  `Ïƒ(z) = 1 / (1 + eâ»á¶»)`  
  where `z = Î¸â‚€ + Î¸â‚xâ‚ + â€¦ + Î¸â‚™xâ‚™`.  
- Cost function: **Log Loss**.  
- Uses a threshold (commonly 0.5) to assign classes.

### ğŸ”¹ Training (Gradient Descent & SGD)
- Iterative optimization to minimize cost functions.  
- **Gradient Descent**: full dataset each update (slower, stable).  
- **Stochastic Gradient Descent (SGD)**: random subset each update (faster, noisier).  
- Learning rate controls step size.  

---

## ğŸ§© Lab Summary (`Logistic_Regression.ipynb`)
- Implemented logistic regression for binary classification.  
- Preprocessing: feature handling, target encoding.  
- Trained using gradient descent.  
- Evaluated using accuracy and log loss.  
- Compared how gradient descent vs. stochastic gradient descent affect convergence.  

---

## ğŸ“Š Skills Learned
- Building regression models for linear, nonlinear, and binary classification tasks.
- Understanding polynomial feature transformations.
- Implementing and interpreting logistic regression.
- Training models using gradient descent & SGD.
- Evaluating models with log loss and classification accuracy.
- Recognizing and addressing **overfitting vs. generalization**.

---

## ğŸš€ Next Steps (Module 3 Preview)
- Classification metrics (accuracy, precision, recall, F1 score).
- Multi-class classification methods.
- Decision Trees, Random Forests, and SVMs.

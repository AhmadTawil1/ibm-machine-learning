# ðŸ“˜ IBM Machine Learning Course â€“ Module 3: Classification & Ensemble Methods

## âœ… Overview
This repository contains my work and learnings from **Module 3** of the IBM Machine Learning course, part of the **IBM AI Engineering Professional Certificate**.  
In this module, I deepened my understanding of **classification algorithms, decision trees, support vector machines (SVMs), K-Nearest Neighbors (KNN), and ensemble methods (Random Forests, XGBoost)**.  
I applied these techniques in labs using real-world datasets (e.g., fraud detection, taxi tips, obesity classification).

---

## ðŸ“‚ Contents
### Labs Completed
1. **Multi-class Classification (Logistic Regression with OvA & OvO)**
   - Implemented one-vs-all and one-vs-one strategies for multi-class classification.
   - Preprocessing: feature scaling, one-hot encoding, label encoding.
   - Stratified train/test split to preserve class distribution.

2. **Decision Trees (Classification)**
   - Built and visualized decision tree classifiers.
   - Understood splitting criteria: **entropy, information gain, Gini impurity**.
   - Applied pruning/stopping rules to prevent overfitting.

3. **Regression Trees (Taxi Tip Dataset)**
   - Applied regression trees to predict continuous outcomes.
   - Used **Mean Squared Error (MSE)** to measure split quality.
   - Compared predicted vs. actual taxi tips.

4. **Support Vector Machines (SVM â€“ Fraud Detection)**
   - Built binary SVM classifiers for imbalanced credit card fraud dataset.
   - Preprocessing pipeline: **standardization â†’ L1 normalization**.
   - Applied **class weights / sample weights** to handle imbalance.
   - Evaluated with **ROC curve & AUC**; noted the importance of PR curves.

5. **K-Nearest Neighbors (KNN)**
   - Implemented KNN classifier with different values of K.
   - Observed **overfitting at small K vs. underfitting at large K**.
   - Preprocessing: scaling features to ensure fair distance calculations.
   - Evaluated with confusion matrix and accuracy.

6. **Random Forests & XGBoost**
   - Understood **bias-variance tradeoff** and ensemble learning.
   - Implemented **Random Forests (bagging)** to reduce variance.
   - Implemented **XGBoost (boosting)** to reduce bias and improve accuracy.
   - Interpreted feature importance from both models.

---

## ðŸ”‘ Key Skills Learned
- **Preprocessing**: scaling, normalization, one-hot encoding, label encoding, stratified sampling.
- **Classification Algorithms**: Logistic Regression, Decision Trees, KNN, SVM.
- **Regression Trees** for continuous predictions.
- **Ensemble Methods**: Bagging (Random Forests), Boosting (XGBoost).
- **Bias-Variance Tradeoff** and how ensembles mitigate it.
- **Model Evaluation**: Confusion Matrix, ROC Curve, AUC, accuracy.

---

## ðŸš€ Outcomes
- Gained hands-on experience with **classification and ensemble methods**.
- Learned how to handle **imbalanced datasets** (class weights, stratification).
- Can now confidently build, evaluate, and tune models to reduce **overfitting and underfitting**.
- Built a foundation for advanced ML techniques in the upcoming modules.

---

## ðŸ§  Reflection
Completing this module gave me the confidence to:
- Choose the right algorithm for a dataset (trees, SVM, KNN, ensembles).
- Balance **bias vs. variance** when tuning models.
- Apply ensemble techniques (Random Forest, XGBoost) to improve predictive power.

This module marks a big step in my AI Engineering journey ðŸš€
